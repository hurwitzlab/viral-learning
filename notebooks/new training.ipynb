{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import warnings\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Lambda\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, mean=None, variance=None):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(1, activation='sigmoid', input_dim=batch_feature_count))\n",
    "    #if mean is not None and variance is not None:\n",
    "    #    model.add(Lambda(lambda x: (x - mean) / variance, input_shape=(input_dim,)))\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, epochs, steps_per_epoch, training_generator, training_args, validation_generator, validation_args):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_training_generator = training_generator(**training_args)\n",
    "    epoch_validation_generator = validation_generator(**validation_args)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        generator=epoch_training_generator,\n",
    "        validation_data=epoch_validation_generator,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=steps_per_epoch\n",
    "    )\n",
    "    print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kmer_batches_combined_h5(bacteria_dset, virus_dset, bacteria_subsample, virus_subsample, half_batch_size):\n",
    "    \"\"\"\n",
    "    Return batches of input and labels from a combined H5 file.\n",
    "\n",
    "    The returned data is shuffled. This is very important. If the\n",
    "    batches are returned with the first half bacteria data and\n",
    "    the second half virus data the models train 'almost perfectly'\n",
    "    and evaluate 'perfectly'.\n",
    "\n",
    "    Arguments:\n",
    "        training_testing_fp: file path of combined training and testing data\n",
    "        bacteria_dset:       H5 dataset bacteria training data\n",
    "        virus_dset:          H5 dataset virus training data\n",
    "        half_batch_size:     each batch will have half_batch_size bacteria samples and half_batch_size virus samples\n",
    "\n",
    "    Yield:\n",
    "        (batch, labels) tuple of (half_batch_size*2, features) and (half_batch_size*2, 1) labels\n",
    "    \"\"\"\n",
    "    print('reading bacteria dataset \"{}\" with shape {}'.format(bacteria_dset.name, bacteria_dset.shape))\n",
    "    print('reading virus dataset \"{}\" with shape {}'.format(virus_dset.name, virus_dset.shape))\n",
    "\n",
    "    batch_size = half_batch_size * 2\n",
    "    batch_count = min(len(bacteria_subsample) // half_batch_size, len(virus_subsample) // half_batch_size)\n",
    "    print('{} batches of {} samples will be yielded in each epoch'.format(batch_count, batch_size))\n",
    "    \n",
    "    # bacteria label is 0\n",
    "    # virus label is 1\n",
    "    labels = np.vstack((np.zeros((half_batch_size, 1)), np.ones((half_batch_size, 1))))\n",
    "\n",
    "    # this is a never ending generator\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        bacteria_sample_groups = grouper(bacteria_subsample, n=half_batch_size)\n",
    "        virus_sample_groups = grouper(virus_subsample, n=half_batch_size)\n",
    "\n",
    "        # note that zip will terminate when it has depleted the shortest of the input iterators\n",
    "        # this is the behavior we want since it happens that some virus testing sets are shorter\n",
    "        # than their associated bacteria testing sets\n",
    "        for bacteria_group, virus_group in zip(bacteria_sample_groups, virus_sample_groups):\n",
    "            # H5 wants a list index to be in ascending order\n",
    "            batch = np.vstack((bacteria_dset[sorted(bacteria_group), :], virus_dset[sorted(virus_group), :]))\n",
    "            yield sklearn.utils.shuffle(batch, labels)\n",
    "        print('generator epoch {} has ended'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go!\n",
    "train_test_fp = '../data/training_testing.h5'\n",
    "batch_size = 50\n",
    "\n",
    "with h5py.File(train_test_fp) as train_test_file:\n",
    "    mean_dset = train_test_file['/clean-bact-vir/training1/extract/kmers/kmer_file1/mean']\n",
    "    variance_dset = train_test_file['/clean-bact-vir/training1/extract/kmers/kmer_file1/variance']\n",
    "\n",
    "    mean = np.zeros(mean_dset.shape)\n",
    "    variance = np.zeros(variance_dset.shape)\n",
    "\n",
    "    mean_dset.read_direct(mean)\n",
    "    variance_dset.read_direct(variance)\n",
    "    variance[variance == 0.0] = 1.0\n",
    "    \n",
    "    model = build_model(input_dim=mean_dset.shape[1], mean=mean, variance=variance)\n",
    "\n",
    "    bacteria_dset = train_test_file['/clean-bact/training1/extract/kmers/kmer_file1']\n",
    "    virus_dset = train_test_file['/clean-vir/training1/extract/kmers/kmer_file1']\n",
    "\n",
    "    training_sample_count = (bacteria_dset.shape[0] // 2) + (virus_dset.shape[0] // 2)\n",
    "    batches_per_epoch =  training_sample_count // batch_size\n",
    "    print('{} training samples'.format(training_sample_count))\n",
    "    print('batch size is {}'.format(batch_size))\n",
    "    print('{} batches in training data'.format(batches_per_epoch))\n",
    "    \n",
    "    epochs = 20\n",
    "    steps_per_epoch = 100\n",
    "    print('{} epochs = {} training samples'.format(epochs, epochs * steps_per_epoch * batch_size))\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        generator=load_kmer_batches_combined_h5(\n",
    "            bacteria_dset=bacteria_dset,\n",
    "            bacteria_subsample=np.random.permutation(bacteria_dset.shape[0] // 2),\n",
    "            virus_dset=virus_dset,\n",
    "            virus_subsample=np.random.permutation(virus_dset.shape[0] // 2),\n",
    "            half_batch_size=batch_size // 2\n",
    "        ),\n",
    "        # there is no advantage to permuting the validation samples\n",
    "        # and there may be a speed advantage to reading them in order\n",
    "        validation_data=load_kmer_batches_combined_h5(\n",
    "            bacteria_dset=bacteria_dset,\n",
    "            bacteria_subsample=np.arange(bacteria_dset.shape[0] // 2) + (bacteria_dset.shape[0] // 2),\n",
    "            virus_dset=virus_dset,\n",
    "            virus_subsample=np.arange(virus_dset.shape[0] // 2) + (virus_dset.shape[0] // 2),\n",
    "            half_batch_size=batch_size // 2\n",
    "        ),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=steps_per_epoch,\n",
    "        workers=2\n",
    "    )\n",
    "    #print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df = pd.DataFrame(data=history.history, index=range(1, epochs + 1))\n",
    "training_performance_df.index.name = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_performance_df.index, training_performance_df.loss, training_performance_df.val_loss)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_performance_df.index, training_performance_df.acc, training_performance_df.val_acc)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
