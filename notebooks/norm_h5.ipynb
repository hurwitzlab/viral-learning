{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h5_fp = '../data/training_testing.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(input_h5_fp, 'r') as input_h5_file:\n",
    "    print(list(input_h5_file['/clean-bact/training1/extract'].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_variance(dsets):\n",
    "    \"\"\"\n",
    "    Given a list of datasets calculate the mean and variance for all rows in all datasets.\n",
    "    \n",
    "    Arguments:\n",
    "        dsets: sequence of datasets with matching column counts\n",
    "        \n",
    "    Returns:\n",
    "        (mean, variance): tuple of mean vector and variance vector\n",
    "    \"\"\"\n",
    "\n",
    "    print('calculating mean and variance for \"{}\"'.format([dset.name for dset in dsets]))\n",
    "    t0 = time.time()\n",
    "    \n",
    "    mean = np.zeros((1, dsets[0].shape[1]))\n",
    "    M2 = np.zeros((1, dsets[0].shape[1]))\n",
    "    count = 0\n",
    "    \n",
    "    for dset in dsets:\n",
    "        # find the right subset size to load without running out of memory\n",
    "        # if dset has more than 10,000 rows use 10,000\n",
    "        # if dset has fewer than 10,000 rows load the whole dset\n",
    "        dsubset = np.zeros((min(10000, dset.shape[0]), dset.shape[1]))\n",
    "        print('  working on \"{}\"'.format(dset.name))\n",
    "        for n in range(0, dset.shape[0], dsubset.shape[0]):\n",
    "            m = min(n + dsubset.shape[0], dset.shape[0])\n",
    "            dset.read_direct(dsubset, source_sel=np.s_[n:m, :])\n",
    "\n",
    "            t00 = time.time()\n",
    "            for i in range(0, dsubset.shape[0]):\n",
    "                count = count + 1 \n",
    "                delta = dsubset[i, :] - mean\n",
    "                mean += delta / count\n",
    "                delta2 = dsubset[i, :] - mean\n",
    "                M2 += delta * delta2\n",
    "            print('    processed slice [{}:{}] {:5.2f}s'.format(n, m, time.time()-t00))\n",
    "\n",
    "    print('  finished mean and variance in {:5.2f}s'.format(time.time()-t0))\n",
    "    # return mean, variance\n",
    "    return (mean, M2/(count - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(input_h5_fp, norm_h5_fp):\n",
    "    dset_paths = []\n",
    "    def find_data(name, obj):\n",
    "        if hasattr(obj, 'dtype'):\n",
    "            print('found dataset \"{}\"'.format(name))\n",
    "            dset_paths.append(obj.name)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with h5py.File(input_h5_fp,  'r', libver='latest', swmr=True) as input_h5_file:\n",
    "        input_h5_file.visititems(find_data)\n",
    "\n",
    "        mean, variance = calculate_mean_variance((\n",
    "            input_h5_file['/clean-bact/training1/extract/kmers'],\n",
    "            input_h5_file['/clean-vir/training1/extract/kmers']))\n",
    "\n",
    "        zero_mean_column_count = len(mean[mean == 0.0])\n",
    "        print('{} column(s) have zero mean'.format(zero_mean_column_count))\n",
    "        zero_var_column_count = len(variance[variance == 0.0])\n",
    "        print('{} column(s) have zero variance'.format(zero_var_column_count))\n",
    "        \n",
    "        with h5py.File(norm_h5_fp, 'w') as norm_h5_file:\n",
    "            print('writing normalized data to \"{}\"'.format(norm_h5_fp))\n",
    "            \n",
    "            mean_dset = norm_h5_file.require_dataset(\n",
    "                name='/mean',\n",
    "                shape=mean.shape,\n",
    "                dtype=mean.dtype,\n",
    "                chunks=mean.shape,\n",
    "                compression='gzip')\n",
    "            mean_dset[:, :] = mean\n",
    "            \n",
    "            variance_dset = norm_h5_file.require_dataset(\n",
    "                name='/variance',\n",
    "                shape=variance.shape,\n",
    "                dtype=variance.dtype,\n",
    "                chunks=variance.shape,\n",
    "                compression='gzip')\n",
    "\n",
    "            variance_dset[:, :] = variance\n",
    "            \n",
    "            for dset_path in dset_paths:\n",
    "                dset = input_h5_file[dset_path]\n",
    "                print('  normalizing \"{}\"'.format(dset.name))\n",
    "                \n",
    "                normalized_dset = norm_h5_file.require_dataset(\n",
    "                    name=dset.name,\n",
    "                    shape=dset.shape,\n",
    "                    dtype=dset.dtype,\n",
    "                    chunks=mean.shape,\n",
    "                    compression='gzip')\n",
    "                \n",
    "                t0 = time.time()\n",
    "                n = 10000\n",
    "                for i in range(0, dset.shape[0], n):\n",
    "                    j = i + n\n",
    "                    # maintain sparsity\n",
    "                    #normalized_dset[i:j, :] = (dset[i:j, :] - mean) / variance\n",
    "                    t00 = time.time()\n",
    "                    normalized_dset[i:j, :] = dset[i:j, :] / variance\n",
    "                    print('  normalized slice {}:{} in {:5.2f}s'.format(i, j, time.time()-t00))\n",
    "                print('normalized \"{}\" in {:5.2f}s'.format(dset.name, time.time()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h5_dp, input_h5_name = os.path.split(input_h5_fp)\n",
    "norm_h5_fp = os.path.join(input_h5_dp, 'norm_' + input_h5_name)\n",
    "normalize_datasets(input_h5_fp, norm_h5_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and variance\n",
    "with h5py.File(input_h5_fp, 'r', libver='latest', swmr=True) as input_h5_file:\n",
    "    dset_paths = []\n",
    "    def find_data(name, obj):\n",
    "        if name.endswith('kmer_file1') or name.endswith('kmer_file2'):\n",
    "            dset_paths.append(obj.name)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    input_h5_file.visititems(find_data)\n",
    "    \n",
    "    for dset_path in dset_paths:\n",
    "        print(dset_path)\n",
    "        mean_dset_path = dset_path + '_mean'\n",
    "        variance_dset_path = dset_path + '_variance'\n",
    "        means = input_h5_file[mean_dset_path]\n",
    "        print('histogramming {} means'.format(np.product(means.shape)))\n",
    "        #print(means[:1])\n",
    "        plt.figure()\n",
    "        plt.title(dset_path)\n",
    "        plt.xlabel('mean')\n",
    "        plt.hist(\n",
    "            means[0, :],\n",
    "            log=True,\n",
    "            bins=50,\n",
    "            range=(0.0, np.max(means)))\n",
    "        variance = input_h5_file[variance_dset_path]\n",
    "        #print(variance[:3, :3])\n",
    "        plt.figure()\n",
    "        plt.title(dset_path)\n",
    "        plt.xlabel('variance')\n",
    "        plt.hist(\n",
    "            variance[0, :],\n",
    "            log=True,\n",
    "            bins=50,\n",
    "            range=(0.0, np.max(variance)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
