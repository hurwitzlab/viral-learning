{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, zip_longest\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_kmer_fp = '../vl/short_clean_bact_kmer_file1.fasta.tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later: any faster if read bytes?\n",
    "with open(short_kmer_fp, 'r') as kmer_file:\n",
    "    header_line = kmer_file.readline()\n",
    "    first_line = kmer_file.readline()\n",
    "    n = 40\n",
    "    print('{} elements of header     : \"{}\"'.format(n, list(header_line[:n])))\n",
    "    print('{} elements of first line : \"{}\"'.format(n, list(first_line[:n])))\n",
    "    \n",
    "    header_columns = header_line.strip().split()\n",
    "    first_line = first_line.strip().split()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the file is tab-delimited.\n",
    "\n",
    "Try just reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(fp, nrows):\n",
    "    df = pd.read_table(filepath_or_buffer=fp, sep='\\t', header=0, index_col=0, nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit load_df(short_kmer_fp, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_py(fp, mode, nrows):\n",
    "    # timeit using open with 'rt':\n",
    "    #   86.9 ms ± 3.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "    # timeit using open with 'r':\n",
    "    #   86 ms ± 690 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "    with open(fp, mode) as f:\n",
    "        header_columns = f.readline().strip().split('\\t')\n",
    "        for i, line in enumerate(f):\n",
    "            if i == nrows:\n",
    "                break\n",
    "            columns = line.strip().split('\\t')\n",
    "            frequencies = [float(f) for f in columns[1:-1]]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit load_py(short_kmer_fp, mode='rt', nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit load_py(short_kmer_fp, mode='r', nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with shuffle=False: 87.5MB -> 3MB\n",
    "# with shuffle=True: 87.5MB -> 14MB\n",
    "def read_df_write_h5(fp, h5_fp):\n",
    "    print('reading \"{}\" with size {:5.2f}MB'.format(fp, os.path.getsize(fp) / 1e6))\n",
    "    print('writing \"{}\"'.format(h5_fp))\n",
    "    with open(fp, 'rt') as f:\n",
    "        column_count = len(f.readline().strip().split('\\t'))\n",
    "        print('file \"{}\" has {} columns'.format(fp, column_count))\n",
    "\n",
    "    t0 = time.time()\n",
    "    initial_dset_rows = 10000\n",
    "    chunksize = 100\n",
    "    with h5py.File(h5_fp, 'w') as h5_f, open(fp, 'rt') as f:\n",
    "        dset_shape = (initial_dset_rows, column_count-2)\n",
    "        dset = h5_f.create_dataset(\n",
    "            '/pd/data',\n",
    "            dset_shape,\n",
    "            dtype=np.float64,\n",
    "            maxshape=(None, dset_shape[1]),\n",
    "            chunks=(1, dset_shape[1]),\n",
    "            shuffle=False,\n",
    "            compression='gzip')\n",
    "\n",
    "        chunk_iter = pd.read_table(\n",
    "            filepath_or_buffer=f,\n",
    "            sep='\\t',\n",
    "            header=0,\n",
    "            index_col=0,\n",
    "            usecols=range(column_count-1),  # skip the first and last columns\n",
    "            chunksize=chunksize)\n",
    "        \n",
    "        si = 0\n",
    "        t00 = time.time()\n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            t11 = time.time()\n",
    "            sj = si + chunk.shape[0]\n",
    "            print('read chunk {} with shape {} in {:5.2f}s ({} rows total)'.format(i, chunk.shape, t11-t00, sj))\n",
    "            dset[si:sj, :] = chunk.values\n",
    "            si = sj\n",
    "            t00 = time.time()\n",
    "            print('  wrote chunk in {:5.2f}s'.format(t00-t11))\n",
    "            \n",
    "        print('read {} rows'.format(si))\n",
    "        print('dataset \"{}\" has shape {}'.format(dset.name, dset.shape))\n",
    "        if sj < dset_shape[0]:\n",
    "            new_shape = (sj, dset.shape[1])\n",
    "            print('resizing dataset from {} to {}'.format(dset.shape, new_shape))\n",
    "            dset.resize(new_shape)\n",
    "\n",
    "    with h5py.File(h5_fp) as h5_f:\n",
    "        dset = h5_f['/pd/data']\n",
    "        print('dataset \"{}\" has shape {}'.format(dset.name, dset.shape))\n",
    "\n",
    "    print('finished writing {} in {:5.2f}s'.format(h5_fp, time.time()-t0))\n",
    "    print('  file size is {:5.2f}MB'.format(os.path.getsize(h5_fp) / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df_write_h5(fp=short_kmer_fp, h5_fp='pd_kmer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(f_, shape_):\n",
    "    chunk_ = np.zeros(shape_)\n",
    "    chunk_i = 0\n",
    "    for line in f_:\n",
    "        chunk_[chunk_i, :] = [float(f) for f in line.rstrip().split('\\t')[1:-1]]\n",
    "        chunk_i += 1\n",
    "        if chunk_i == shape_[0]:\n",
    "            # end of a chunk!\n",
    "            yield chunk_\n",
    "            chunk_i = 0\n",
    "\n",
    "    if chunk_i > 0:\n",
    "        # yield a partial chunk\n",
    "        yield chunk_[:chunk_i, :]\n",
    "\n",
    "def read_py_write_h5(fp_list, dset, chunksize):\n",
    "    t0 = time.time()\n",
    "    si = 0\n",
    "    for fp in fp_list:\n",
    "        with open(fp, 'rt') as f:\n",
    "            print('reading \"{}\" with size {:5.2f}MB'.format(fp, os.path.getsize(fp) / 1e6))\n",
    "            header_line = f.readline()\n",
    "            print('  header : \"{}\"'.format(header_line[:30]))\n",
    "            column_count = len(header_line.strip().split('\\t'))\n",
    "            print('  header has {} columns'.format(column_count))\n",
    "\n",
    "            t00 = time.time()\n",
    "            for i, chunk in enumerate(read_chunk(f, shape_=(chunksize, column_count-2))):\n",
    "                t11 = time.time()\n",
    "                sj = si + chunk.shape[0]\n",
    "                print('read chunk {} with shape {} in {:5.2f}s ({} rows total)'.format(i, chunk.shape, t11-t00, sj))\n",
    "                dset[si:sj, :] = chunk\n",
    "                si = sj\n",
    "                t00 = time.time()\n",
    "                print('  wrote chunk in {:5.2f}s'.format(t00-t11))\n",
    "\n",
    "            print('read {} rows'.format(si))\n",
    "            print('dataset \"{}\" has shape {}'.format(dset.name, dset.shape))\n",
    "            if sj < dset.shape[0]:\n",
    "                new_shape = (sj, dset.shape[1])\n",
    "                print('resizing dataset from {} to {}'.format(dset.shape, new_shape))\n",
    "                dset.resize(new_shape)\n",
    "\n",
    "    print('finished writing {} in {:5.2f}s'.format(h5_fp, time.time()-t0))\n",
    "    print('  file size is {:5.2f}MB'.format(os.path.getsize(h5_fp) / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_fp = 'py_kmer.h5'\n",
    "column_count = 32768\n",
    "with h5py.File(h5_fp, 'w') as h5_f: \n",
    "    dset = h5_f.create_dataset(\n",
    "        '/python/data',\n",
    "        (10000, column_count),\n",
    "        dtype=np.float64,\n",
    "        maxshape=(None, column_count),\n",
    "        chunks=(1, column_count),\n",
    "        shuffle=False,\n",
    "        compression='gzip')\n",
    "\n",
    "    print('writing \"{}\"'.format(h5_fp))\n",
    "    read_py_write_h5(fp_list=(short_kmer_fp, ), dset=dset, chunksize=1000)\n",
    "\n",
    "with h5py.File(h5_fp) as h5_f:\n",
    "    dset = h5_f['/python/data']\n",
    "    print('  dataset \"{}\" has shape {}'.format(dset.name, dset.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
