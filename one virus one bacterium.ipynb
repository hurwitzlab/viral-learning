{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Dropout, Embedding, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a viral genome mycobacterium_phage_shipwreck\n",
    "with open('genome_virus.fasta', 'rt') as viral_genome_file:\n",
    "    viral_genome_header = viral_genome_file.readline()\n",
    "    viral_genome_buffer = StringIO()\n",
    "    for line in viral_genome_file.readlines():\n",
    "        viral_genome_buffer.write(line.strip())\n",
    "viral_genome = viral_genome_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_genome_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('viral genome has {} bp'.format(len(viral_genome)))\n",
    "viral_genome[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reads(genome, read_count, read_length):\n",
    "    \"\"\"Yield 'read_count' random reads of length 'read_length' from 'genome'.\n",
    "    \"\"\"\n",
    "    for n in random.randint(low=0, high=len(genome)-read_length+1, size=read_count):\n",
    "        yield genome[n:n+read_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small example\n",
    "for r in generate_reads('ACGTAC', read_count=5, read_length=5):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure a Tokenizer to convert A,C,G,T to 1,2,3,4\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts('ACGT')\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.document_count)\n",
    "print(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these parameters for building the model and layers\n",
    "mini_batch_size = 10\n",
    "input_vector_length = 100\n",
    "input_vector_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(label, genome, read_count, read_length):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    genome_reads = list(generate_reads(\n",
    "        genome, read_count=read_count, read_length=read_length))\n",
    "    #print(np.asarray(viral_reads)[:5])\n",
    "    genome_sequences = tokenizer.texts_to_sequences(texts=genome_reads)\n",
    "    print(np.asarray(genome_sequences)[:5])\n",
    "    genome_sequence_array = np.asarray(genome_sequences)\n",
    "    print(genome_sequence_array)\n",
    "    genome_sequence_label_array = label * np.ones((read_count,1))\n",
    "\n",
    "    return genome_sequence_array, genome_sequence_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_training_data, viral_training_labels = generate_training_data(\n",
    "    1.0, genome=viral_genome, read_count=input_vector_count, read_length=input_vector_length)\n",
    "print('viral training data shape        : {}'.format(viral_training_data.shape))\n",
    "print('viral training data labels shape : {}'.format(viral_training_labels.shape))\n",
    "viral_validation_data, viral_validation_labels = generate_training_data(\n",
    "    1.0, genome=viral_genome, read_count=input_vector_count, read_length=input_vector_length)\n",
    "print('viral validation data shape        : {}'.format(viral_validation_data.shape))\n",
    "print('viral validation data labels shape : {}'.format(viral_validation_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read bacterial genome and create training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCF_000195955.2_ASM19595v2_genomic.fna.gz\n",
    "with gzip.open('genome_bacterium.fna.gz', 'rt') as bacterial_genome_file:\n",
    "    bacterial_genome_header = bacterial_genome_file.readline()\n",
    "    print(bacterial_genome_header)\n",
    "    bacterial_genome_buffer = StringIO()\n",
    "    for i, line in enumerate(bacterial_genome_file.readlines()):\n",
    "        if i > 1000:\n",
    "            break\n",
    "        else:\n",
    "            bacterial_genome_buffer.write(line.strip())\n",
    "bacterial_genome = bacterial_genome_buffer.getvalue()\n",
    "print(bacterial_genome[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacterial_training_data, bacterial_training_labels = generate_training_data(\n",
    "    0.0, genome=bacterial_genome, read_count=input_vector_count, read_length=input_vector_length)\n",
    "print('bacterial training data shape        : {}'.format(bacterial_training_data.shape))\n",
    "print('bacterial training data labels shape : {}'.format(bacterial_training_labels.shape))\n",
    "bacterial_validation_data, bacterial_validation_labels = generate_training_data(\n",
    "    0.0, genome=bacterial_genome, read_count=input_vector_count, read_length=input_vector_length)\n",
    "print('bacterial validation data shape        : {}'.format(bacterial_validation_data.shape))\n",
    "print('bacterial validation data labels shape : {}'.format(bacterial_validation_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 4+1\n",
    "maxlen = input_vector_length\n",
    "batch_size = mini_batch_size\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a binary classification problem\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.vstack((viral_training_data, bacterial_training_data))\n",
    "print(training_data.shape)\n",
    "training_labels = np.vstack((viral_training_labels, bacterial_training_labels))\n",
    "print(training_labels.shape)\n",
    "\n",
    "validation_data = np.vstack((viral_validation_data, bacterial_validation_data))\n",
    "print(validation_data.shape)\n",
    "validation_labels = np.vstack((viral_validation_labels, bacterial_validation_labels))\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_data, training_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
